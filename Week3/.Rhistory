table(df.sal)
table(df.sal$type_employer)
head(select(df.sal, type_employer))
str(df.sal)
select(df.sal, filter(type_employer=='Without-pay' | type_employer=='Never-Worked'))
select(df.sal, filter(df.sal, type_employer=='Without-pay' | type_employer=='Never-Worked'))
select(df.sal, filter(df.sal, type_employer=='Without-pay'))
head(filter(df.sal, type_employer=='Without-pay'))
head(filter(df.sal, type_employer=='Without-pay' || type_employer=='Never-worked'))
head(filter(df.sal, type_employer=='Without-pay' | type_employer=='Never-worked'))
head(select(df.sal, type_employer))
impute_employerType <- function(employerType){
for (i in 1:length(employerType)){
out[i] <- 'Unemployed'
}
return(out)
}
df.sal$type_employer <- filter(df.sal, type_employer=='Without-pay' |
type_employer=='Never-worked')
df.sal$type_employer <- filter(df.sal$type_employer, type_employer=='Without-pay' |
type_employer=='Never-worked')
df.sal$type_employer <- impute_employerType(filter(df.sal$type_employer, type_employer=='Without-pay' |
type_employer=='Never-worked'))
str(df.sal$type_employer)
df.sal$type_employer <- as.character(df.sal$type_employer)
str(df.sal$type_employer)
df.sal$type_employer <- impute_employerType(filter(df.sal$type_employer, type_employer=='Without-pay' |
type_employer=='Never-worked'))
df.sal$type_employer['Never-worked'] <- df.sal$type_employer['Unemployed']
df.sal$type_employer['Never-worked']
unique(df.sal$type_employer)
df.sal$type_employer['Never-worked']
df.sal$type_employer== 'Never-worked'
isTRUE(df.sal$type_employer== 'Never-worked')
unique(df.sal$type_employer)
isTRUE(df.sal$type_employer== 'Never-worked')
df.sal$type_employer== 'Never-worked'
isTRUE(df.sal$type_employer== 'Never-worked')
install.packages('ISLR')
library(ISLR)
str(Caravan)
install.packages('tm')
install.packages('twitteR')
install.packages('wordcloud')
install.packages('RColorBrewer')
install.packages('e1017')
install.packages('class')
library(tm)
library(twitteR)
library(wordcloud)
library(RColorBrewer)
library(e1017)
library(class)
install.packages('e1017')
#Load Library
library(tm)
ckey <- 'QoaruWiInsk8xI3kvvcX1hTnh'
skey <- '11qRYXbKOI2ETgqTscxyqaPs0iTO3RoHOe0uXgtMPfOpjJw1Iy'
token <- '1123478819774640128-qrra1l03G998N4Fk2nDUPBe1mu6FQQ'
sectoken <- 'WGUlNLelrbUpPgFEwIPz80YamGSzpN0m5vAN225sYgidO'
#Connect to twitter
setup_twitter_oauth(ckey, skey, token, sectoken)
#Connect to twitter
setup_twitter_oauth(ckey, skey, token, sectoken)
# get Soccer Tweets
soccer.tweets <- searchTwitter('soccer', n=1000, lang = 'en')
View(soccer.tweets)
View(soccer.tweets)
# get Soccer Tweets
soccer.tweets <- searchTwitter('soccer', n=1000, lang = 'en')
soccer.text <- sapply(soccer.tweets, function(x) x$getText())
soccer.text <- sapply(soccer.tweets, function(x) x$getText())
#Clean Text Data, remove all text which are not part of UTF-8 and ASCII
soccer.text <- iconv(soccer.text, 'UTF-8', 'ASCII')
soccer.corpus <- Corpus(VectorSource(soccer.text))
removeNumbers=TRUE, tolower())
# Document term Matrix, remove common english words,
# word='soccer' and common english words also lower all the text
term.doc.matrix <- TermDocumentMatrix(soccer.corpus,
control = list(removePunctuation=TRUE,
stopwords=c('soccer', stopwords('english')),
removeNumbers=TRUE, tolower()))
# Document term Matrix, remove common english words,
# word='soccer' and common english words also lower all the text
term.doc.matrix <- TermDocumentMatrix(soccer.corpus,
control = list(removePunctuation=TRUE,
stopwords=c('soccer', stopwords('english')),
removeNumbers=TRUE, tolower=TRUE))
head(soccer.corpus)
View(soccer.corpus)
View(soccer.corpus)
soccer.corpus[["1"]]
soccer.corpus[["1"]]
# COnvert object into a matrix
term.doc.matrix<- as.matrix(term.doc.matrix)
#Get Word Counts
word.freq <- sort(rowSums(term.doc.matrix), decreasing = TRUE)
head(term.doc.matrix)
head(word.freq)
dm <- data.frame(word=names(word.freq), freq=word.freq)
head(dm)
# Create the wordcloud
wordcloud(dm$word, dm$freq, random.color = FALSE, colors = brewer.pal(8, 'Dark2'))
# Create the wordcloud
wordcloud(dm$word, dm$freq, random.color = FALSE, colors = brewer.pal(8, 'Dark2'))
# Create the wordcloud
wordcloud(dm$word, dm$freq, random.color = FALSE, colors = brewer.pal(8, 'Dark2'))
# Create the wordcloud
wordcloud(dm$word, dm$freq, random.color = FALSE, colors = brewer.pal(8, 'Dark2'))
# Create the wordcloud
wordcloud(dm$word, dm$freq, random.color = FALSE, colors = brewer.pal(8, 'Dark2'))
# geting Soccer Tweets from Twitter
soccer.tweets <- searchTwitter('pyton', n=1000, lang = 'en')
#Retriving text data from Tweets
soccer.text <- sapply(soccer.tweets, function(x) x$getText())
#Clean Text Data, remove all text which are not part of UTF-8 and ASCII
soccer.text <- iconv(soccer.text, 'UTF-8', 'ASCII')
soccer.corpus <- Corpus(VectorSource(soccer.text))
# Document term Matrix, remove common english words,
# word='soccer' and common english words also lower all the text
term.doc.matrix <- TermDocumentMatrix(soccer.corpus,
control = list(removePunctuation=TRUE,
stopwords=c('pyton', 'http', stopwords('english')),
removeNumbers=TRUE, tolower=TRUE))
# COnvert object into a matrix
term.doc.matrix<- as.matrix(term.doc.matrix)
#Get Word Counts
word.freq <- sort(rowSums(term.doc.matrix), decreasing = TRUE)
dm <- data.frame(word=names(word.freq), freq=word.freq)
head(dm)
# Create the wordcloud
wordcloud(dm$word, dm$freq, random.color = FALSE, colors = brewer.pal(8, 'Dark2'))
# Create the wordcloud
wordcloud(dm$word, dm$freq, random.color = FALSE, colors = brewer.pal(8, 'Dark2'))
head(dm)
# geting Soccer Tweets from Twitter
soccer.tweets <- searchTwitter('Islam', n=1000, lang = 'en')
#Retriving text data from Tweets
soccer.text <- sapply(soccer.tweets, function(x) x$getText())
#Clean Text Data, remove all text which are not part of UTF-8 and ASCII
soccer.text <- iconv(soccer.text, 'UTF-8', 'ASCII')
soccer.corpus <- Corpus(VectorSource(soccer.text))
# Document term Matrix, remove common english words,
# word='soccer' and common english words also lower all the text
term.doc.matrix <- TermDocumentMatrix(soccer.corpus,
control = list(removePunctuation=TRUE,
stopwords=c('Islam', 'http', stopwords('english')),
removeNumbers=TRUE, tolower=TRUE))
# COnvert object into a matrix
term.doc.matrix<- as.matrix(term.doc.matrix)
#Get Word Counts
word.freq <- sort(rowSums(term.doc.matrix), decreasing = TRUE)
dm <- data.frame(word=names(word.freq), freq=word.freq)
head(dm)
# Create the wordcloud
wordcloud(dm$word, dm$freq, random.color = FALSE, colors = brewer.pal(8, 'Dark2'))
# Create the wordcloud
#wordcloud(dm$word, dm$freq, random.color = FALSE, colors = brewer.pal(8, 'Dark2'))
wordcloud(dm$word, dm$freq, random.color = FALSE)
# Create the wordcloud
#wordcloud(dm$word, dm$freq, random.color = FALSE, colors = brewer.pal(8, 'Dark2'))
wordcloud(dm$word, dm$freq, random.color = FALSE)
# Create the wordcloud
#wordcloud(dm$word, dm$freq, random.color = FALSE, colors = brewer.pal(8, 'Dark2'))
wordcloud(dm$word, dm$freq, random.color = FALSE)
# Create the wordcloud
#wordcloud(dm$word, dm$freq, random.color = FALSE, colors = brewer.pal(8, 'Dark2'))
wordcloud(dm$word, dm$freq, random.color = FALSE)
# Create the wordcloud
#wordcloud(dm$word, dm$freq, random.color = FALSE, colors = brewer.pal(8, 'Dark2'))
wordcloud(dm$word, dm$freq, random.color = T)
sum(dm$freq)
# geting Soccer Tweets from Twitter
soccer.tweets <- searchTwitter('Soccer', n=1000, lang = 'en')
#Retriving text data from Tweets
soccer.text <- sapply(soccer.tweets, function(x) x$getText())
#Clean Text Data, remove all text which are not part of UTF-8 and ASCII
soccer.text <- iconv(soccer.text, 'UTF-8', 'ASCII')
soccer.corpus <- Corpus(VectorSource(soccer.text))
# Document term Matrix, remove common english words,
# word='soccer' and common english words also lower all the text
term.doc.matrix <- TermDocumentMatrix(soccer.corpus,
control = list(removePunctuation=TRUE,
stopwords=c('soccer', 'http', stopwords('english')),
removeNumbers=TRUE, tolower=TRUE))
# COnvert object into a matrix
term.doc.matrix<- as.matrix(term.doc.matrix)
#Get Word Counts
word.freq <- sort(rowSums(term.doc.matrix), decreasing = TRUE)
dm <- data.frame(word=names(word.freq), freq=word.freq)
head(dm)
sum(dm$freq)
# Create the wordcloud
#wordcloud(dm$word, dm$freq, random.color = FALSE, colors = brewer.pal(8, 'Dark2'))
wordcloud(dm$word, dm$freq, random.color = T)
# Create the wordcloud
#wordcloud(dm$word, dm$freq, random.color = FALSE, colors = brewer.pal(8, 'Dark2'))
wordcloud(dm$word, dm$freq, random.color = TRUE)
# Create the wordcloud
#wordcloud(dm$word, dm$freq, random.color = FALSE, colors = brewer.pal(8, 'Dark2'))
wordcloud(dm$word, dm$freq, random.color = TRUE)
?iconv
?Corpus
# geting Soccer Tweets from Twitter
soccer.tweets <- searchTwitter('cricket', n=1000, lang = 'en')
#Retriving text data from Tweets
soccer.text <- sapply(soccer.tweets, function(x) x$getText())
#Clean Text Data, remove all text which are not part of UTF-8 and ASCII
soccer.text <- iconv(soccer.text, 'UTF-8', 'ASCII')
soccer.corpus <- Corpus(VectorSource(soccer.text))
# Document term Matrix, remove common english words,
# word='soccer' and common english words also lower all the text
term.doc.matrix <- TermDocumentMatrix(soccer.corpus,
control = list(removePunctuation=TRUE,
stopwords=c('cricket', 'http', stopwords('english')),
removeNumbers=TRUE, tolower=TRUE))
# COnvert object into a matrix
term.doc.matrix<- as.matrix(term.doc.matrix)
#Get Word Counts
word.freq <- sort(rowSums(term.doc.matrix), decreasing = TRUE)
dm <- data.frame(word=names(word.freq), freq=word.freq)
head(dm)
sum(dm$freq)
# Create the wordcloud
#wordcloud(dm$word, dm$freq, random.color = FALSE, colors = brewer.pal(8, 'Dark2'))
wordcloud(dm$word, dm$freq, random.color = TRUE)
# Create the wordcloud
#wordcloud(dm$word, dm$freq, random.color = FALSE, colors = brewer.pal(8, 'Dark2'))
wordcloud(dm$word, dm$freq, random.color = F)
sum(dm$freq)
head(dm)
dm
# geting Soccer Tweets from Twitter
soccer.tweets <- searchTwitter('IPL', n=1000, lang = 'en')
#Retriving text data from Tweets
soccer.text <- sapply(soccer.tweets, function(x) x$getText())
#Clean Text Data, remove all text which are not part of UTF-8 and ASCII
soccer.text <- iconv(soccer.text, 'UTF-8', 'ASCII')
soccer.corpus <- Corpus(VectorSource(soccer.text))
# Document term Matrix, remove common english words,
# word='soccer' and common english words also lower all the text
term.doc.matrix <- TermDocumentMatrix(soccer.corpus,
control = list(removePunctuation=TRUE,
stopwords=c('cricket', 'http', stopwords('english')),
removeNumbers=TRUE, tolower=TRUE))
# COnvert object into a matrix
term.doc.matrix<- as.matrix(term.doc.matrix)
#Get Word Counts
word.freq <- sort(rowSums(term.doc.matrix), decreasing = TRUE)
dm <- data.frame(word=names(word.freq), freq=word.freq)
# Create the wordcloud
#wordcloud(dm$word, dm$freq, random.color = FALSE, colors = brewer.pal(8, 'Dark2'))
wordcloud(dm$word, dm$freq, random.color = F)
# geting Soccer Tweets from Twitter
soccer.tweets <- searchTwitter('IPL', n=10000, lang = 'en')
#Retriving text data from Tweets
soccer.text <- sapply(soccer.tweets, function(x) x$getText())
#Clean Text Data, remove all text which are not part of UTF-8 and ASCII
soccer.text <- iconv(soccer.text, 'UTF-8', 'ASCII')
soccer.corpus <- Corpus(VectorSource(soccer.text))
# Document term Matrix, remove common english words,
# word='soccer' and common english words also lower all the text
term.doc.matrix <- TermDocumentMatrix(soccer.corpus,
control = list(removePunctuation=TRUE,
stopwords=c('cricket', 'http', stopwords('english')),
removeNumbers=TRUE, tolower=TRUE))
# COnvert object into a matrix
term.doc.matrix<- as.matrix(term.doc.matrix)
#Get Word Counts
word.freq <- sort(rowSums(term.doc.matrix), decreasing = TRUE)
dm <- data.frame(word=names(word.freq), freq=word.freq)
dm
head(dm)
sum(dm$freq)
# Create the wordcloud
#wordcloud(dm$word, dm$freq, random.color = FALSE, colors = brewer.pal(8, 'Dark2'))
wordcloud(dm$word, dm$freq, random.color = F)
apropos('mea')
apropos('tex')
apropos('text analysis')
apropos('analysis')
apropos('apri')
apropos('pnt')
apropos('sapp')
history(#)
history(#)
history()
options()
install.packages('vcd')
example(Arthritis)
library(vcd)
example(Arthritis)
head(mtcars)
head(mtcars$mpg)
head(scale(mtcars$mpg))
summary(mtcars$mpg))
summary(mtcars$mpg)
sd(mtcars$mpg)
head(scale(mtcars$mpg) * sd(mtcars$mpg) + mean(mtcars$mpg))
t(mtcars)
version
R.version
help()
head(scale(mtcars$mpg) * sd(mtcars$mpg) + mean(mtcars$mpg))
install.packages('vcd')
library(vcd)
barplot(height)
barplot(height)
barplot(Arthritis$Treatment)
barplot(Arthritis$Improved)
barplot(Arthritis$Age)
boxplot(Arthritis$Age)
count <- Arthritis$Improved
barplot(Arthritis$count)
count <- table(Arthritis$Improved)
barplot(Arthritis$count)
barplot(Arthritis$count, main="Simple Bar Plot",
xlab="Improvement", ylab="Frequency")
count <- as.matrix(Arthritis$Improved)
barplot(Arthritis$count, main="Simple Bar Plot",
xlab="Improvement", ylab="Frequency")
class(count)
count <- as.table(Arthritis$Improved)
class(count)
count <- table(Arthritis$Improved)
class(count)
barplot(Arthritis$count, main="Simple Bar Plot",
xlab="Improvement", ylab="Frequency")
count <- c(42, 14,28)
count <- colnames(c('None', 'Some', 'Marked'))
count
count <- c(42, 14,28)
count
count <- c('None', 'Some', 'Marked')
count
count <- c(42, 14,28)
barplot(Arthritis$count, main="Simple Bar Plot",
xlab="Improvement", ylab="Frequency")
barplot(count, main="Simple Bar Plot",
xlab="Improvement", ylab="Frequency")
barplot(count, main="Simple Bar Plot",
xlab="Improvement", ylab="Frequency", horiz = TRUE)
barplot(Arthritis$Improved, main="Simple Bar Plot",
xlab="Improvement", ylab="Frequency", horiz = TRUE)
class(Arthritis$Improved)
barplot(Arthritis$Improved, main="Simple Bar Plot",
xlab="Improvement", ylab="Frequency", horiz = TRUE)
head(Arthritis)
summary(Arthritis)
library(ggplot2)
library(dplyr)
library(ggplot2)
library(dplyr)
library(ggplot2)
library(dplyr)
library(ggplot2)
library(dplyr)
install.packages('ggplot2')
install.packages('dplyr')
library(ggplot2)
library(dplyr)
install.packages('vcd')
library(vcd)
barplot(Arthritis$Improved, main="Simple Bar Plot",
xlab="Improvement", ylab="Frequency", horiz = TRUE)
head(Arthritis)
barplot(Arthritis$Improved, main="Simple Bar Plot",
xlab="Improvement", ylab="Frequency", horiz = TRUE)
install.packages('ggplot2')
install.packages('dplyr')
#load Packages
library(ggplot2)
library(dplyr)
head(Arthritis)
summary(Arthritis)
class(Arthritis$Improved)
barplot(Arthritis$Improved, main="Simple Bar Plot",
xlab="Improvement", ylab="Frequency", horiz = TRUE)
hist(mtcars$mpg)
install.packages("rJava")
options(java.parameters = "-Xmx8192m" )
options(mc.cores = 3)
library(ggplot2); library(slam);
filePathSep <- "/"
fileNameSep <- "."
#swiftKeyDirectory <- ".\\data\\Coursera-SwiftKey"
swiftKeyDirectory <- "../data"
#finalDirectory <- paste(swiftKeyDirectory, "final", sep = filePathSep)
finalDirectory <- paste(swiftKeyDirectory, sep = filePathSep)
outputDirectory <- paste(swiftKeyDirectory, "output", sep = filePathSep)
localesAvail <- c("de_DE", "en_US", "fi_FI", "ru_RU")
locales <- localesAvail[2]
contexts <- c("blogs", "news", "twitter")
fileExt <- "txt"
set.seed(55669)
source("week3-sampleData.R")
getFileInfo <- function(directory) {
df <- data.frame(name = c(), size = c())
for (locale in locales) {
for (context in contexts) {
fileName <- paste(locale, context, fileExt, sep = fileNameSep)
fullQualifiedFileName <- paste(directory, locale, fileName, sep = filePathSep)
if (file.exists(fullQualifiedFileName) == TRUE) {
fInfo <- file.info(fullQualifiedFileName)
fileSizeInMb <- paste(round(fInfo$size / 1024 / 1024, 2), "MB")
df <- rbind(df, data.frame(name = fileName, size = fileSizeInMb))
} else {
stop("File not found!")
}
}
}
df
}
makeFqnOutputFilePath <- function(locale, context) {
localeDirectory <- paste(outputDirectory, locale, sep = filePathSep)
dir.create(localeDirectory, showWarnings = FALSE, recursive = TRUE)
fileName <- paste(locale, context, fileExt, sep = fileNameSep)
fqnOutputFileName <- paste(localeDirectory, fileName, sep = filePathSep)
fqnOutputFileName
}
makeReducedData <- function(fileName, factor) {
connection <- file(fileName, "rb")
contents <- readLines(connection, encoding = "UTF-8", skipNul = TRUE)
newContents <- sample(contents, length(contents) * factor)
on.exit(close(connection))
newContents
}
writeDataToFile <- function(fileName, data, printFileName = FALSE) {
write(data, file = fileName) # over write file
if(printFileName == TRUE) print(fileName)
}
makeSampleFiles <- function(sampleSize = 0.01) {
for (locale in locales) {
for (context in contexts) {
fileName <- paste(locale, context, fileExt, sep = fileNameSep)
fullQualifiedFileName <- paste(finalDirectory, locale, fileName, sep = filePathSep)
if (file.exists(fullQualifiedFileName) == TRUE) {
writeDataToFile(
makeFqnOutputFilePath(locale, context),
makeReducedData(fullQualifiedFileName, sampleSize))
} else {
stop("File not found!")
}
}
}
}
source("week3-sampleData.R")
setwd("~/Documents/GIT/ datasciencecoursera/Capstone Project/Capstone-Project/Week3")
source("week3-sampleData.R")
makeSampleFiles(0.01) # 3%
source("week3-constructCorpus.R")
enUsOutputDirectory <- paste(outputDirectory, locales, sep = filePathSep)
ovid <- makeCorpus(enUsOutputDirectory)
ovid <- transformCorpus(ovid)
ovid <- tagDocumentWithId(ovid)
save(ovid, file="corpus.RData"); rm(ovid)
source("week3-nGramMaker.R")
load("corpus.RData")
ngrams <- makeNGrams(ovid)
ngrams <- makeNGrams(ovid)
library(rJava)
install.packages("rJava")
library(rJava)
ngrams <- makeNGrams(ovid)
oneGram <- ngrams[[1]]; biGram <- ngrams[[2]]; triGram <- ngrams[[3]];
ngrams <- makeNGrams(ovid)
ngrams <- makeNGrams(ovid)
ngrams <- makeNGrams(ovid)
library(rJava)
source("./week3-buildingPredictiveModel.R")
load("oneGram.RData"); load("biGram.RData"); load("triGram.RData");
install.packages("parallel")
install.packages("foreach")
install.packages("doParallel")
ovid <- makeCorpus(enUsOutputDirectory)
options(java.parameters = "-Xmx8192m" )
options(mc.cores = 3)
library(ggplot2); library(slam);
filePathSep <- "/"
fileNameSep <- "."
#swiftKeyDirectory <- ".\\data\\Coursera-SwiftKey"
swiftKeyDirectory <- "../data"
#finalDirectory <- paste(swiftKeyDirectory, "final", sep = filePathSep)
finalDirectory <- paste(swiftKeyDirectory, sep = filePathSep)
outputDirectory <- paste(swiftKeyDirectory, "output", sep = filePathSep)
localesAvail <- c("de_DE", "en_US", "fi_FI", "ru_RU")
locales <- localesAvail[2]
contexts <- c("blogs", "news", "twitter")
fileExt <- "txt"
set.seed(55669)
source("week3-sampleData.R")
makeSampleFiles(0.01) # 3%
source("week3-constructCorpus.R")
enUsOutputDirectory <- paste(outputDirectory, locales, sep = filePathSep)
ovid <- makeCorpus(enUsOutputDirectory)
ovid <- transformCorpus(ovid)
ovid <- tagDocumentWithId(ovid)
save(ovid, file="corpus.RData"); rm(ovid)
source("week3-nGramMaker.R")
load("corpus.RData")
ngrams <- makeNGrams(ovid)
install.packages("markovchain")
ovid <- makeCorpus(enUsOutputDirectory)
ovid <- transformCorpus(ovid)
ovid <- tagDocumentWithId(ovid)
save(ovid, file="corpus.RData"); rm(ovid)
source("week3-nGramMaker.R")
load("corpus.RData")
ngrams <- makeNGrams(ovid)
ngrams <- makeNGrams(ovid)
oneGram <- ngrams[[1]]; biGram <- ngrams[[2]]; triGram <- ngrams[[3]];
ngrams <- makeNGrams(ovid)
